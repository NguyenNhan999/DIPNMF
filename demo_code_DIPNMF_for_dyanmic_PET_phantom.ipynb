{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import imageio\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list=\"2\", allow_growth=True))\n",
    "\n",
    "#size of dynamic sinogram \n",
    "sino_size = [182, 182, 30]\n",
    "sn_size = 182*182\n",
    "\n",
    "#size of dynamic PET\n",
    "I1 = 128\n",
    "I2 = 128\n",
    "T  = 30\n",
    "\n",
    "# Load data\n",
    "lmat = scipy.io.loadmat('simulation_data_SN20.mat')\n",
    "Y = lmat['sinogram_noisy'] # Sinogram matrix with Poisson noise (SNR=20dB), size = (182*182, 30)\n",
    "P = lmat['PET_image'] # True dynamic PET image matrix, size = (128*128, 30)\n",
    "R = lmat['R'] # Radon transform matrix (P in the paper), size = (182*182, 128*128)\n",
    "\n",
    "# define Radon transform matrix as SparseTensor for TensorFlow\n",
    "coo = R.tocoo()\n",
    "indices = np.mat([coo.row, coo.col]).transpose()\n",
    "Rtf = tf.SparseTensor(indices, coo.data.astype(np.float32), coo.shape)\n",
    "\n",
    "# Load smoothing matrix (H=L'L in the paper)\n",
    "lmat = scipy.io.loadmat('smoothing_matrix.mat')\n",
    "H = lmat['H']\n",
    "H = H.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "# Define Unet for deep image prior\n",
    "pad_para = [[0,0],[1,1],[1,1],[0,0]]\n",
    "pad_type = 'reflect'\n",
    "\n",
    "FS = [128]*5\n",
    "\n",
    "kd = 3\n",
    "ks = 1\n",
    "ku = 3\n",
    "\n",
    "def Unet(data, train=False):\n",
    "    d1 = tf.pad(data, pad_para, pad_type)\n",
    "    #d1 = tf.pad(d1, pad_para, pad_type)\n",
    "    d1 = tf.layers.conv2d(d1,filters=FS[0],kernel_size=[kd,kd],padding=\"valid\",strides=(2,2))\n",
    "    d1 = tf.layers.batch_normalization(d1, training=train)\n",
    "    d1 = tf.nn.leaky_relu(d1)\n",
    "    d1 = tf.pad(d1, pad_para, pad_type)\n",
    "    d1 = tf.layers.conv2d(d1,filters=FS[0],kernel_size=[kd,kd],padding=\"valid\",strides=(1,1))\n",
    "    d1 = tf.layers.batch_normalization(d1, training=train)\n",
    "    d1 = tf.nn.leaky_relu(d1)\n",
    "    s1 = tf.layers.conv2d(d1,filters=4,kernel_size=[ks,ks],padding=\"same\",strides=(1,1))\n",
    "    s1 = tf.layers.batch_normalization(s1)\n",
    "    s1 = tf.nn.leaky_relu(s1)\n",
    "    d2 = tf.pad(d1, pad_para, pad_type)\n",
    "    d2 = tf.layers.conv2d(d2,filters=FS[1],kernel_size=[kd,kd],padding=\"valid\",strides=(2,2))\n",
    "    d2 = tf.layers.batch_normalization(d2, training=train)\n",
    "    d2 = tf.nn.leaky_relu(d2)\n",
    "    d2 = tf.pad(d2, pad_para, pad_type)\n",
    "    d2 = tf.layers.conv2d(d2,filters=FS[1],kernel_size=[kd,kd],padding=\"valid\",strides=(1,1))\n",
    "    d2 = tf.layers.batch_normalization(d2, training=train)\n",
    "    d2 = tf.nn.leaky_relu(d2)\n",
    "    s2 = tf.layers.conv2d(d2,filters=4,kernel_size=[ks,ks],padding=\"same\",strides=(1,1))\n",
    "    s2 = tf.layers.batch_normalization(s2, training=train)\n",
    "    s2 = tf.nn.leaky_relu(s2)\n",
    "    d3 = tf.pad(d2, pad_para, pad_type)\n",
    "    d3 = tf.layers.conv2d(d3,filters=FS[2],kernel_size=[kd,kd],padding=\"valid\",strides=(2,2))\n",
    "    d3 = tf.layers.batch_normalization(d3, training=train)\n",
    "    d3 = tf.nn.leaky_relu(d3)\n",
    "    d3 = tf.pad(d3, pad_para, pad_type)\n",
    "    d3 = tf.layers.conv2d(d3,filters=FS[2],kernel_size=[kd,kd],padding=\"valid\",strides=(1,1))\n",
    "    d3 = tf.layers.batch_normalization(d3, training=train)\n",
    "    d3 = tf.nn.leaky_relu(d3)\n",
    "    s3 = tf.layers.conv2d(d3,filters=4,kernel_size=[ks,ks],padding=\"same\",strides=(1,1))\n",
    "    s3 = tf.layers.batch_normalization(s3, training=train)\n",
    "    s3 = tf.nn.leaky_relu(s3)\n",
    "    d4 = tf.pad(d3, pad_para, pad_type)\n",
    "    d4 = tf.layers.conv2d(d4,filters=FS[3],kernel_size=[kd,kd],padding=\"valid\",strides=(2,2))\n",
    "    d4 = tf.layers.batch_normalization(d4, training=train)\n",
    "    d4 = tf.nn.leaky_relu(d4)\n",
    "    d4 = tf.pad(d4, pad_para, pad_type)\n",
    "    d4 = tf.layers.conv2d(d4,filters=FS[3],kernel_size=[kd,kd],padding=\"valid\",strides=(1,1))\n",
    "    d4 = tf.layers.batch_normalization(d4, training=train)\n",
    "    d4 = tf.nn.leaky_relu(d4)\n",
    "    s4 = tf.layers.conv2d(d4,filters=4,kernel_size=[ks,ks],padding=\"same\",strides=(1,1))\n",
    "    s4 = tf.layers.batch_normalization(s4, training=train)\n",
    "    s4 = tf.nn.leaky_relu(s4)\n",
    "    d5 = tf.pad(d4, pad_para, pad_type)\n",
    "    d5 = tf.layers.conv2d(d5,filters=FS[4],kernel_size=[kd,kd],padding=\"valid\",strides=(2,2))\n",
    "    d5 = tf.layers.batch_normalization(d5, training=train)\n",
    "    d5 = tf.nn.leaky_relu(d5)\n",
    "    d5 = tf.pad(d5, pad_para, pad_type)\n",
    "    d5 = tf.layers.conv2d(d5,filters=FS[4],kernel_size=[kd,kd],padding=\"valid\",strides=(1,1))\n",
    "    d5 = tf.layers.batch_normalization(d5, training=train)\n",
    "    d5 = tf.nn.leaky_relu(d5)\n",
    "    s5 = tf.layers.conv2d(d5,filters=4,kernel_size=[ks,ks],padding=\"same\",strides=(1,1))\n",
    "    s5 = tf.layers.batch_normalization(s5, training=train)\n",
    "    s5 = tf.nn.leaky_relu(s5)\n",
    "    u4 = tf.concat([d5, s5],axis=3)\n",
    "    u4 = tf.layers.batch_normalization(u4, training=train)\n",
    "    u4 = tf.pad(u4, pad_para, pad_type)\n",
    "    u4 = tf.layers.conv2d(u4,filters=FS[4],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    u4 = tf.layers.batch_normalization(u4, training=train)    \n",
    "    u4 = tf.nn.leaky_relu(u4)\n",
    "    u4 = tf.layers.conv2d(u4,filters=FS[4],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    u4 = tf.layers.batch_normalization(u4, training=train)\n",
    "    u4 = tf.nn.leaky_relu(u4)\n",
    "    u4 = tf.image.resize_bilinear(u4,[u4.shape[1]*2, u4.shape[2]*2])\n",
    "    u3 = tf.concat([u4, s4],axis=3)\n",
    "    u3 = tf.layers.batch_normalization(u3, training=train)\n",
    "    u3 = tf.pad(u3, pad_para, pad_type)\n",
    "    u3 = tf.layers.conv2d(u3,filters=FS[3],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    u3 = tf.layers.batch_normalization(u3, training=train)\n",
    "    u3 = tf.nn.leaky_relu(u3)\n",
    "    u3 = tf.layers.conv2d(u3,filters=FS[3],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    u3 = tf.layers.batch_normalization(u3, training=train)\n",
    "    u3 = tf.nn.leaky_relu(u3)\n",
    "    u3 = tf.image.resize_bilinear(u3,[u3.shape[1]*2, u3.shape[2]*2])\n",
    "    u2 = tf.concat([u3, s3],axis=3)\n",
    "    u2 = tf.layers.batch_normalization(u2, training=train)\n",
    "    u2 = tf.pad(u2, pad_para, pad_type)\n",
    "    u2 = tf.layers.conv2d(u2,filters=FS[2],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    u2 = tf.layers.batch_normalization(u2, training=train)\n",
    "    u2 = tf.nn.leaky_relu(u2)\n",
    "    u2 = tf.layers.conv2d(u2,filters=FS[2],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    u2 = tf.layers.batch_normalization(u2, training=train)\n",
    "    u2 = tf.nn.leaky_relu(u2)\n",
    "    u2 = tf.image.resize_bilinear(u2,[u2.shape[1]*2, u2.shape[2]*2])\n",
    "    u1 = tf.concat([u2, s2],axis=3)\n",
    "    u1 = tf.layers.batch_normalization(u1, training=train)\n",
    "    u1 = tf.pad(u1, pad_para, pad_type)\n",
    "    u1 = tf.layers.conv2d(u1,filters=FS[1],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    u1 = tf.layers.batch_normalization(u1, training=train)\n",
    "    u1 = tf.nn.leaky_relu(u1)\n",
    "    u1 = tf.layers.conv2d(u1,filters=FS[1],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    u1 = tf.layers.batch_normalization(u1, training=train)\n",
    "    u1 = tf.nn.leaky_relu(u1)\n",
    "    u1 = tf.image.resize_bilinear(u1,[u1.shape[1]*2, u1.shape[2]*2])\n",
    "    u0 = tf.concat([u1, s1],axis=3)\n",
    "    u0 = tf.layers.batch_normalization(u0, training=train)\n",
    "    u0 = tf.pad(u0, pad_para, pad_type)\n",
    "    u0 = tf.layers.conv2d(u0,filters=FS[0],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    u0 = tf.layers.batch_normalization(u0, training=train)\n",
    "    u0 = tf.nn.leaky_relu(u0)\n",
    "    u0 = tf.layers.conv2d(u0,filters=FS[0],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    u0 = tf.layers.batch_normalization(u0, training=train)\n",
    "    u0 = tf.nn.leaky_relu(u0)\n",
    "    out = tf.image.resize_bilinear(u0,[u0.shape[1]*2, u0.shape[2]*2])\n",
    "    out = tf.pad(out, pad_para, pad_type)\n",
    "    out = tf.layers.conv2d(out,filters=FS[0],kernel_size=[ku,ku],padding=\"valid\",strides=(1,1))\n",
    "    out = tf.layers.batch_normalization(out, training=train)\n",
    "    out = tf.nn.leaky_relu(out)\n",
    "    out = tf.layers.conv2d(out,filters=FS[0],kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    out = tf.layers.batch_normalization(out, training=train)\n",
    "    out = tf.nn.leaky_relu(out)\n",
    "    out = tf.layers.conv2d(out,filters=1,kernel_size=[1,1],padding=\"same\",strides=(1,1))\n",
    "    return tf.nn.sigmoid(out)\n",
    "\n",
    "# Define KL divergence\n",
    "def kl_divergence(p, q):\n",
    "    return tf.reduce_sum(q - p*tf.log(q))\n",
    "\n",
    "# Define quadratic variation penalty for smooth temporal bases\n",
    "def QVnorm(Xs,Hs):\n",
    "    return tf.trace(tf.matmul(tf.matmul(Xs,Hs),Xs,transpose_b=True))\n",
    "\n",
    "# Update operation for smooth temporal bases (X = B' in paper)\n",
    "ONE = np.ones((sn_size,T), dtype='float32')\n",
    "def update_X(Xs,Ys,As,Hs,x_rate):\n",
    "    top = tf.maximum(tf.matmul(As ,tf.divide(tf.maximum(Ys,1e-10),tf.maximum(tf.matmul(As,Xs),1e-10) ), transpose_a=True), 0.0) + tf.maximum(-tf.matmul(Xs,Hs),0.0) + 1e-5\n",
    "    bot = tf.maximum(tf.matmul(As ,ONE, transpose_a=True), 0.0 ) + tf.maximum(tf.matmul(Xs,Hs),0.0) + 1e-5\n",
    "    mul = tf.pow(tf.divide(top,bot),x_rate)\n",
    "    return tf.maximum(tf.multiply(Xs,mul),1e-5)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import imageio\n",
    "import numpy.matlib\n",
    "\n",
    "# Probrem definitions\n",
    "BATCH_SIZE = 1\n",
    "train_size = 1\n",
    "test_size  = 1\n",
    "\n",
    "code_depth = 32\n",
    "\n",
    "# placeholder of input noise of Unet\n",
    "train_data_node  = tf.placeholder(tf.float32,shape=(BATCH_SIZE,I1,I2,code_depth))\n",
    "\n",
    "# placeholder of observed noisy Sinogram\n",
    "train_labels_node = tf.placeholder(tf.float32,shape=(sn_size,T))# placeholder of observed noisy Sinogram\n",
    "\n",
    "# constant value of KL divergence\n",
    "Y = np.maximum(Y,1e-10)\n",
    "klc = np.sum((Y * np.log(Y)) - Y)\n",
    "\n",
    "# Generated model of reconstructed sinogram\n",
    "r = 3 # number of spatio-temporal bases (rank of NMF)\n",
    "X_node  = tf.placeholder(tf.float32,shape=(r,T))\n",
    "RA_node = tf.placeholder(tf.float32,shape=(sn_size,r))\n",
    "H_node  = tf.placeholder(tf.float32,shape=(T,T))\n",
    "x_rate_node = tf.placeholder(tf.float32,shape=())\n",
    "X = update_X(X_node,train_labels_node,RA_node,H_node,x_rate_node)\n",
    "\n",
    "A = Unet(train_data_node, True)\n",
    "for ri in range(r-1):\n",
    "    A = tf.concat([A, Unet(train_data_node, True)],axis=3)\n",
    "A   = tf.reshape(A,[128*128,r])\n",
    "RA  = tf.sparse_tensor_dense_matmul(Rtf,A) + 1e-10\n",
    "RAX = tf.matmul(RA,X_node) + 1e-10 # reconstructed sinogram\n",
    "\n",
    "\n",
    "# Cost function\n",
    "alpha = 0.01 # hyper parameter for exclusiveness penalty\n",
    "beta  = 5.0  # hyper parameter for smoothness penalty\n",
    "\n",
    "loss = klc + kl_divergence(train_labels_node,RAX)\n",
    "lp   = alpha * tf.reduce_sum(tf.norm(A, axis=1, ord=0.5) ** 2)\n",
    "qv   = QVnorm(X_node,H_node)\n",
    "cost = loss + lp + 0.5*qv\n",
    "\n",
    "# Optimizer setting\n",
    "batch = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                 # Base learning rate.\n",
    "    batch*BATCH_SIZE,     # Current index into the dataset.\n",
    "    100,                  # Decay step.\n",
    "    0.98,                 # Decay rate.\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=batch)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig  = plt.figure(figsize=(12,4),dpi=100,tight_layout=True)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "ax241= fig.add_subplot(241)\n",
    "ax242= fig.add_subplot(242)\n",
    "ax243= fig.add_subplot(243)\n",
    "ax244= fig.add_subplot(244)\n",
    "ax245= fig.add_subplot(245)\n",
    "ax246= fig.add_subplot(246)\n",
    "ax247= fig.add_subplot(247)\n",
    "ax248= fig.add_subplot(248)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start algorithm\n",
    "num_iter = 10000\n",
    "cost_hist = np.zeros([num_iter,3])\n",
    "snr_hist  = np.zeros([num_iter,1])\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialization\n",
    "z = np.random.uniform(0.0,0.1,(1,I1,I2,32)) # input noise for Unet\n",
    "z = z.astype('float32')\n",
    "Xest = np.matlib.repmat(np.mean(Y, axis=0),r,1) + np.random.normal(0.0,1.0,(r,T))\n",
    "Xest = Xest.astype('float32')\n",
    "x_rate = 1.0\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    print('Iinitialized!')\n",
    "    print('Iterations     :: Cost function    :: SNR value :: learning rate :: elapsed time :: x_rate ')\n",
    "    for step in range(0,num_iter):\n",
    "        noise = np.random.normal(0.0, 1.0/30.0,(1,I1,I2,32))# small noise for robust learning\n",
    "        batch_data = z + noise\n",
    "        batch_label= Y\n",
    "        feed_dict = {train_data_node: batch_data,train_labels_node: batch_label, X_node:Xest, H_node:beta*H}\n",
    "        # Update A\n",
    "        _, cost_value, loss_value, lp_value, RAest, Aest = sess.run([optimizer, cost, loss, lp, RA, A], feed_dict)\n",
    "        # Normalize A\n",
    "        for ri in range(r):\n",
    "            RAest[:,ri] = RAest[:,ri] / np.max(Aest[:,ri])\n",
    "        # Update X (=B'), i_B = 10\n",
    "        for nn in range(10):\n",
    "            x_dict = {X_node:Xest, train_labels_node: batch_label, RA_node:RAest, H_node:beta*H, x_rate_node:x_rate}\n",
    "            Xest2 = sess.run(X, x_dict)\n",
    "            if np.max((Xest2/Xest)) > 1.1:\n",
    "                x_rate = x_rate*0.1\n",
    "            else:\n",
    "                if np.min((Xest2/Xest)) < 0.9:\n",
    "                    x_rate = x_rate*0.1\n",
    "                else:\n",
    "                    Xest=Xest2\n",
    "        # Update learning rate\n",
    "        l_rate = sess.run(learning_rate)\n",
    "        # Record cost functions\n",
    "        cost_hist[step,0] = cost_value\n",
    "        cost_hist[step,1] = loss_value\n",
    "        cost_hist[step,2] = lp_value\n",
    "        # Evaluate denoising score (SNR)\n",
    "        feed_dict_test = {train_data_node: z, train_labels_node: batch_label, X_node:Xest, RA_node:RAest, H_node:beta*H}\n",
    "        Aest, RAXest, cost_value = sess.run([A,RAX,cost],feed_dict_test)\n",
    "        AXest = np.matmul(Aest,Xest)\n",
    "        snr_value = 10.0*np.log10(np.sum(P**2) / np.sum((P - AXest)**2))\n",
    "        snr_hist[step,0] = snr_value\n",
    "        # Show optimization behaviors\n",
    "        if ((step < 100) & (step % 1 == 0)) | ((1000 > step > 100) & (step % 10 == 0)) | ((step > 1000) & (step % 100 == 0)):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time   = time.time()\n",
    "            print('Step %d / %d :: %f :: %f :: %f :: %f :: %f ' % (step,num_iter,cost_value,snr_value,l_rate,elapsed_time,x_rate))\n",
    "            ax245.clear()\n",
    "            ax245.imshow(np.reshape(Aest[:,0],[128, 128]).T, cmap='jet', vmin=0.0, vmax=1.0)\n",
    "            ax245.set_title(\"Spatial basis 1\")\n",
    "            ax246.clear()\n",
    "            ax246.imshow(np.reshape(Aest[:,1],[128, 128]).T, cmap='jet', vmin=0.0, vmax=1.0)\n",
    "            ax246.set_title(\"Spatial basis 2\")\n",
    "            ax247.clear()\n",
    "            ax247.imshow(np.reshape(Aest[:,2],[128, 128]).T, cmap='jet', vmin=0.0, vmax=1.0)\n",
    "            ax247.set_title(\"Spatial basis 3\")\n",
    "            ax248.clear()\n",
    "            ax248.plot(Xest.T)\n",
    "            ax248.legend(['Basis 1','Basis 2','Basis 3'])\n",
    "            ax248.set_title(\"Temporal bases\")\n",
    "            ax243.clear()\n",
    "            ax243.imshow(np.reshape(AXest[:,15],[128, 128]).T, cmap='jet')\n",
    "            ax243.set_title(\"Rec. PET (15th frame)\")\n",
    "            ax242.clear()\n",
    "            ax242.imshow(np.reshape(RAXest[:,15],[182, 182]).T, cmap='jet', vmin=0.0, vmax=350000.0)\n",
    "            ax242.set_title(\"Rec. sinogram (15th frame)\")\n",
    "            ax241.clear()\n",
    "            ax241.imshow(np.reshape(Y[:,15],[182, 182]).T, cmap='jet', vmin=0.0, vmax=350000.0)\n",
    "            ax241.set_title(\"Noisy sinogram (15th frame)\")\n",
    "            ax244.clear()\n",
    "            ax244.plot(snr_hist[0:step])\n",
    "            ax244.set_title(\"SNR of dynamic PET\")\n",
    "            fig.canvas.draw()\n",
    "    print('finished')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
